\documentclass[12pt]{report}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage[style=authoryear,backend=biber]{biblatex}
\addbibresource{references.bib}


\onehalfspacing
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}

\begin{document}

\title{Developing a Transformer-Based Automated Essay Scoring System with Explainability Considerations}
\author{Oscar Dos Santos Nunes}
\date{March 2025 - September 2025}
\maketitle

\tableofcontents
\newpage

\chapter{Introduction}

\section{Background and Context}

\section{Background and Context}

Automated Essay Scoring (AES) systems have been in development for over fifty years, evolving from statistical and rule-based approaches into increasingly sophisticated machine learning and deep 
learning technologies. Early developments such as Project Essay Grade (PEG) \parencite{page1966imminence} and e-rater \parencite{attali2006e_rater} relied on handcrafted features, including high-level 
syntactic and lexical features. While these methods demonstrated some success and saw real-world application, they often struggled to capture the semantic and contextual depth found in student writing 
\parencite{shermis2013handbook}.

With the growth of more data-focused approaches in natural language processing (NLP), AES has increasingly adopted machine learning techniques. Traditional models such as support vector machines (SVM), 
random forests, and ridge regression were trained on handcrafted features to predict human-marked essay scores. However, these models were still limited by their reliance on manually created inputs, 
which creates constraints with scalability and generalisation across writing prompts and student population groups.

The growth of deep learning, particularly with transformer-based models, has significantly advanced AES research. Pre-trained language models such as BERT \parencite{devlin2019bert} and RoBERTa 
\parencite{liu2019roberta} have demonstrated impressive results across various NLP tasks, thanks to their ability to model contextual relationships within text. When compared to more typical 
machine learning techniques, studies using BERT for AES have shown substantial gains in accuracy, especially when using benchmark datasets such as the ASAP (Automated Student Assessment Prize) 
corpus \parencite{taghipour2016neural}.

Despite these advancements, a number of obstacles still exist. Deep learning models frequently act as ``black boxes'', providing little to no room for interpretation. This is a major drawback for 
AES, as user trust and educational integration rely on the reasoning behind a score. In recent years, there has also been growing concern about algorithmic bias, particularly when scoring responses 
from under-represented student groups or non-native English speakers \parencite{blodgett2020language}. These issues raise questions about fairness, generalisability, and the ethical use of AES in 
high-stakes contexts.

Furthermore, much of current AES work focuses on holistic scoring without exploring the formative aspects of feedback, rubric-based scoring, or the integration of multi-modal inputs. Expandability 
methods such as SHAP \parencite{lundberg2017unified} or attention-based visualisations have been actively researched as a means of improving model transparency. Hybrid systems that combine neural 
and symbolic components have also shown promise in enhancing interpretability and control.

This project builds on these advancements by developing an AES model using deep learning, evaluating its performance and fairness, and investigating potential pathways for expansion. Through comparing 
results against traditional baselines and considering explainability and ethical implications, this work aims to contribute to research seeking to make AES systems more robust, transparent, and 
educationally useful.





\section{Problem Statement}
\section{Aims and Objectives}
\section{Research Questions}
\section{Significance of the Study}
\section{Dissertation Structure Overview}

\chapter{Literature Review}
\section{Historical Evolution of Automated Essay Scoring}
\section{Rule-Based and Traditional ML Methods}
\section{Deep Learning and Transformer-Based Models}
\section{Datasets for AES}
\section{Evaluation Metrics in AES}
\section{Challenges in AES}
\section{Gaps in Existing Research}

\chapter{Methodology}
\section{Research Design and Approach}
\section{Data Collection and Preprocessing}
\subsection{Dataset Selection and Description}
\subsection{Tokenisation and Cleaning}
\subsection{Label Normalisation}
\section{Model Architecture and Implementation}
\subsection{Baseline Models}
\subsection{Transformer Fine-Tuning}
\subsection{Training Configuration}
\section{Evaluation Strategy}
\subsection{Metrics Used}
\subsection{Cross-Validation Setup}
\section{Tools and Libraries}
\section{Ethical Considerations}

\chapter{Experiments and Results}
\section{Baseline Performance}
\section{Transformer Performance}
\section{Error Analysis}
\section{Model Explainability}
\section{Fairness and Bias Analysis}
\section{Comparison with Human Raters}

\chapter{Discussion}
\section{Interpretation of Results}
\section{Contributions to the Field}
\section{Limitations}
\section{Implications for Educational Technology}

\chapter{Conclusion}
\section{Summary of Findings}
\section{Answers to Research Questions}
\section{Future Work}


\printbibliography


\appendix
\chapter{Code Snippets}
\chapter{Gantt Chart / Timeline}
\chapter{Additional Tables or Figures}
\chapter{Ethics Approval Form}

\end{document}

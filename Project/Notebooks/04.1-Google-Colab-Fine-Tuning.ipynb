{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.52.4 datasets==3.6.0 evaluate==0.4.3 huggingface-hub==0.32.6 --quiet\n",
        "!pip install optuna --quiet\n",
        "!pip install --upgrade scikit-learn --quiet\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J9fy1bQyKU8D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_from_disk\n",
        "import evaluate\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "8ck1FAgSIGYv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"tokenised_asap_split.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"tokenised_asap_split\")\n",
        "\n",
        "print(os.listdir(\"tokenised_asap_split\"))  # Should show the extracted folders\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76Sli3xbIIyC",
        "outputId": "bc05dcd2-5d13-498f-91f3-81115a828040"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__MACOSX', 'tokenised_asap_split']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"tokenised_asap_split/tokenised_asap_split\"  # Adjust if different\n",
        "dataset_dict = load_from_disk(dataset_path)\n",
        "\n",
        "# Rename column for Trainer compatibility\n",
        "dataset_dict = dataset_dict.rename_column(\"score_scaled\", \"labels\")\n",
        "\n",
        "print(dataset_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klyx57qvIKfA",
        "outputId": "0588c682-5cd6-4c62-d454-a1ef41a212e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['essay', 'labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 9651\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['essay', 'labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 1206\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['essay', 'labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 1207\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_tN7bHkIMlT",
        "outputId": "6e665b02-96f1-48fb-df6d-a2af28d2fc40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],\n",
        "    eval_steps=500,           # Number of steps between evaluations (won't be used if no eval strategy)\n",
        "    save_strategy=\"steps\",    # Save checkpoint every `save_steps`\n",
        "    save_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\"\n",
        "    # No evaluation_strategy or load_best_model_at_end\n",
        ")\n"
      ],
      "metadata": {
        "id": "h3YBfpLHIOLu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = evaluate.load(\"mse\")\n",
        "r2 = evaluate.load(\"r_squared\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.squeeze()\n",
        "    return {\n",
        "        \"mse\": mse.compute(predictions=preds, references=labels)[\"mse\"],\n",
        "        \"r2\": r2.compute(predictions=preds, references=labels)[\"r_squared\"]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rh4M6OiQIaek"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"validation\"],  # use validation split if available\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#trainer.train()\n"
      ],
      "metadata": {
        "id": "YrLBGMGmIcXB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load metric once\n",
        "mse = evaluate.load(\"mse\")\n",
        "r2 = evaluate.load(\"r_squared\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.squeeze()\n",
        "    mse_val = np.mean((preds - labels) ** 2)\n",
        "    r2_val = 1 - np.sum((labels - preds) ** 2) / np.sum((labels - np.mean(labels)) ** 2)\n",
        "    return {\n",
        "        \"mse\": mse_val,\n",
        "        \"r2\": r2_val\n",
        "    }\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        report_to=[],\n",
        "        eval_steps=500,           # Number of steps between evaluations (won't be used if no eval strategy)\n",
        "        save_strategy=\"steps\",    # Save checkpoint every `save_steps`\n",
        "        save_steps=500,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\"\n",
        "        # No evaluation_strategy or load_best_model_at_end\n",
        "    )\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_dict[\"train\"],\n",
        "        eval_dataset=dataset_dict[\"validation\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result[\"eval_mse\"]\n",
        "\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=25)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "gA4cY-wTgoUm",
        "outputId": "969ed7f2-26f0-4a6c-b5de-5fc37968a740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-11 16:09:40,412] A new study created in memory with name: no-name-d0b1becc-651c-44b1-bdd8-d193f8c2c1c6\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='3621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  54/3621 00:18 < 20:51, 2.85 it/s, Epoch 0.04/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}